"""
=================================================================
Test with permutations the significance of a classification score
=================================================================

In order to test if a classification score is significative a technique
in repeating the classification procedure after randomizing, permuting,
the labels. The p-value is then given by the percentage of runs for
which the score obtained is greater than the classification score
obtained in the first place.

"""

# Author:  Alexandre Gramfort <alexandre.gramfort@inria.fr>
# License: BSD 3 clause

print(__doc__)

import numpy as np
import pylab as pl

from sklearn.svm import SVC
from sklearn.cross_validation import StratifiedKFold, permutation_test_score
from sklearn import datasets


##############################################################################
# Loading a dataset
#iris = datasets.load_iris()
#X = iris.data
#y = iris.target

coupleValuesEntire = np.array([
[1.513,0.842,1.404,1.152,0.032,0.5,0.083,0.511,0.928,2.557,2.5,0.032,0.083,0.965,0.022,1.458,0.769,30.5,8,0.477],
[3.41,3.3,4.851,1.854,0.789,0.86,5.11,0.683,1.423,1.505,1.896,0.789,5.11,1.868,-0.231,4.131,2.056,2474.5,459,0.674],
[5.088,1.977,5.932,1.829,3.317,0.749,5.381,0.809,1.166,1.186,1.097,3.317,5.381,0.484,0.078,5.51,1.725,3713.5,625,0.743],
[4.383,1.764,4.253,1.457,1.568,0.702,2.313,0.626,0.97,1.02,1.144,1.568,2.313,0.389,-0.114,4.318,1.125,2772.5,593,0.584],
[3.881,1.894,3.931,1.074,1.651,0.64,3.451,0.547,1.013,1.064,1.245,1.651,3.451,0.737,-0.157,3.906,1.138,2235.5,494,0.566],
[3.96,0.746,3.322,1.193,5.477,0.524,1.297,0.521,0.839,0.664,0.668,5.477,1.297,-1.44,-0.006,3.641,0.755,2128,532,0.5],
[2.082,2.061,3.116,2.319,0.364,0.633,0.58,0.728,1.496,1.58,1.375,0.364,0.58,0.465,0.139,2.599,1.861,1168.5,249,0.587],
[1.764,1.286,1.158,1.275,0.184,0.525,0.079,0.5,0.657,0.451,0.473,0.184,0.079,-0.841,-0.048,1.461,0.821,16,4,0.5],
[4.251,2.681,3.072,1.885,1.133,0.812,0.853,0.593,0.723,0.633,0.867,1.133,0.853,-0.283,-0.314,3.662,1.593,1976,410,0.602],
[4.513,2.182,3.85,2.182,1.831,0.734,1.102,0.7,0.853,0.774,0.811,1.831,1.102,-0.507,-0.047,4.181,1.75,2457.5,477,0.644],
[0.767,0.649,2.651,0.98,0.018,0.5,0.442,0.501,3.458,17.019,17,0.018,0.442,3.181,0.001,1.709,0.676,59,15,0.492],
[4.021,1.147,3.207,1.678,0.97,0.636,0.474,0.63,0.798,0.648,0.654,0.97,0.474,-0.715,-0.008,3.614,1.106,1848,429,0.538],
[4.949,2.565,5.018,1.178,2.248,0.811,21.531,0.639,1.014,1.087,1.381,2.248,21.531,2.26,-0.239,4.983,1.539,3255,592,0.687],
[3.225,1.667,3.322,1.983,0.571,0.642,0.7,0.671,1.03,1.184,1.134,0.571,0.7,0.205,0.044,3.274,1.28,1558.5,367,0.531],
[4.37,0.761,4.819,1.03,12.315,0.561,12.073,0.621,1.103,1.106,0.998,12.315,12.073,-0.02,0.103,4.595,0.793,3231.5,695,0.581],
[4.755,1.176,4.818,1.248,5.145,0.637,6.815,0.635,1.013,1.038,1.042,5.145,6.815,0.281,-0.004,4.786,0.932,3330.5,682,0.61],
[5.715,1.287,5.33,2.644,43.812,0.722,1.525,0.923,0.933,0.79,0.618,43.812,1.525,-3.358,0.246,5.522,1.901,3571,587,0.76],
[3.668,1.238,4.836,1.811,1.495,0.561,2.716,0.714,1.318,1.552,1.22,1.495,2.716,0.597,0.241,4.252,1.322,2730,575,0.593],
[5.668,2.612,5.841,1.304,2.81,0.876,13.733,0.755,1.031,1.09,1.264,2.81,13.733,1.587,-0.148,5.755,1.504,3691.5,618,0.747],
[4.602,1.466,5.358,2.597,3.017,0.653,1.959,0.883,1.164,1.192,0.881,3.017,1.959,-0.432,0.301,4.98,1.591,3274.5,594,0.689],
[4.237,1.212,3.793,1.384,1.548,0.631,1.267,0.613,0.895,0.895,0.92,1.548,1.267,-0.2,-0.028,4.015,1.106,2218,473,0.586],
[4.507,0.833,3.714,0.906,8.244,0.587,0.729,0.586,0.824,0.472,0.473,8.244,0.729,-2.425,-0.001,4.11,0.604,2793.5,664,0.526],
[4.297,0.741,6.95,1.054,13.979,0.548,64.364,0.877,1.618,1.686,1.055,13.979,64.364,1.527,0.469,5.623,0.665,4029.5,715,0.704],
[5.929,2.135,6.54,1.296,4.15,0.844,54.462,0.825,1.103,1.191,1.219,4.15,54.462,2.574,-0.023,6.235,1.461,4438,703,0.789],
[4.429,0.832,6.222,1.186,14.021,0.566,59.083,0.784,1.405,1.458,1.053,14.021,59.083,1.438,0.325,5.326,0.85,3825.5,717,0.667],
[5.457,1.425,4.369,1.509,14.298,0.706,2.44,0.639,0.801,0.687,0.759,14.298,2.44,-1.768,-0.1,4.913,1.082,3444,684,0.629],
[6.093,0.895,5.774,1.804,718,0.762,6.49,0.781,0.948,0.89,0.868,718,6.49,-4.706,0.025,5.934,1.095,4257.5,717,0.742],
[3.389,0.751,7.305,1.166,0.97,0.506,718,0.913,2.156,3.658,2.028,0.97,718,6.607,0.59,5.347,0.728,3839,718,0.668],
[3.885,0.802,4.568,1.72,2.587,0.537,2.042,0.694,1.176,1.203,0.931,2.587,2.042,-0.236,0.257,4.226,0.971,2602,567,0.574],
[4.29,1.536,5.106,2.39,1.899,0.646,2.784,0.782,1.19,1.36,1.123,1.899,2.784,0.383,0.191,4.698,1.481,3011.5,579,0.65]])



X = coupleValuesEntire
matLevel = np.array([0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2])
y = matLevel
valueExtract = []
for j in range(len(coupleValuesEntire)):
   valueExtract.append([coupleValuesEntire[j][0],coupleValuesEntire[j][1],coupleValuesEntire[j][2],coupleValuesEntire[j][3],\
                      coupleValuesEntire[j][8],coupleValuesEntire[j][10]])
    
### 0,1,2,3,8,10 gives 73% on SVC
### 0,1,2,3,8,10,19 gives 73% on SVC
### 0,1,2,3,8,10,14 gives 73% on SVC

X = coupleValuesEntire[:,0:8]

print X

n_classes = np.unique(y).size

# Some noisy data not correlated
random = np.random.RandomState(seed=0)
E = random.normal(size=(len(X), 2200))

# Add noisy data to the informative features for make the task harder
X = np.c_[X, E]

svm = SVC(kernel='linear')
cv = StratifiedKFold(y, 2)

score, permutation_scores, pvalue = permutation_test_score(
    svm, X, y, score_func="accuracy", cv=cv, n_permutations=100, n_jobs=1)

#score, permutation_scores, pvalue = permutation_test_score(
    #svm, X, y, cv=cv, n_permutations=100, n_jobs=1)

print("Classification score %s (pvalue : %s)" % (score, pvalue))

###############################################################################
# View histogram of permutation scores
pl.hist(permutation_scores, 20, label='Permutation scores')
ylim = pl.ylim()
# BUG: vlines(..., linestyle='--') fails on older versions of matplotlib
#pl.vlines(score, ylim[0], ylim[1], linestyle='--',
#          color='g', linewidth=3, label='Classification Score'
#          ' (pvalue %s)' % pvalue)
#pl.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle='--',
#          color='k', linewidth=3, label='Luck')
pl.plot(2 * [score], ylim, '--g', linewidth=3,
        label='Classification Score'
        ' (pvalue %s)' % pvalue)
pl.plot(2 * [1. / n_classes], ylim, '--k', linewidth=3, label='Luck')

pl.ylim(ylim)
pl.legend()
pl.xlabel('Score')
pl.show()
