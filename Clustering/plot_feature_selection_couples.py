"""
===============================
Univariate Feature Selection
===============================

An example showing univariate feature selection.

Noisy (non informative) features are added to the iris data and
univariate feature selection is applied. For each feature, we plot the
p-values for the univariate feature selection and the corresponding
weights of an SVM. We can see that univariate feature selection
selects the informative features and that these have larger SVM weights.

In the total set of features, only the 4 first ones are significant. We
can see that they have the highest score with univariate feature
selection. The SVM assigns a large weight to one of these features, but also
Selects many of the non-informative features.
Applying univariate feature selection before the SVM
increases the SVM weight attributed to the significant features, and will
thus improve classification.
"""
print(__doc__)

import numpy as np
import pylab as pl

from sklearn import datasets, svm
from sklearn.feature_selection import SelectPercentile, f_classif

###############################################################################
#   data

coupleValuesEntire= np.array([
[0.959463,-1.987,-0.828,-0.926091,-1.34972,3.48232,-0.792,5.344,-0.231,0.498,2.633,-1.114,-1.873,-1.191,0.653,1.48853,-1.935,-0.502,-1.05,-1.31692,3.25402,-0.444817,0.703,-0.323,-0.046,1.707,-1.362,-1.915,-2.017,0.857,1.315,-2.171,-0.79,-1.03864,-1.36024,-1.43399,1.8067,3.28903,-0.974085,-0.974085,2.304,-0.328,2.285,2.175],
[-2.35353,0.103,-0.303,1.15767,0.614423,0.739048,0.364,-0.27,-0.183,-0.219,0.033,-0.272,-0.412,0.915,-1.089,-0.907093,-0.433,-0.065,0.548,0.39496,1.00842,0.234345,-0.313,-0.284,-0.22,0.189,-1.059,-0.796,-0.019,-0.064,-1.78,-0.212,-0.043,0.871346,0.333227,0.584732,1.4552,0.842645,0.0826414,0.0826414,-0.32,-0.248,-0.252,0.093],
[0.353861,-2.559,-1.103,-1.49236,-1.34972,3.55521,-0.872,5.433,-0.231,1.008,2.64,-1.114,-1.873,-2.18,-0.047,-0.0461656,-1.151,-0.827,-0.214,-1.34029,2.18963,1.65441,1.995,-0.325,-0.233,1.973,-1.362,-1.915,-0.564,-0.129,0.171,-1.978,-1.008,-0.84857,-1.36459,-1.43979,2.25516,3.24601,-0.770868,-0.770868,4.883,-0.334,0.406,2.413],
[0.567603,-0.532,2.659,0.525647,0.676232,1.5342,-0.783,-0.267,0.095,0.157,0.161,1.831,1.39,-2.235,1.54,0.665035,0.233,0.824,1.255,1.13113,-0.289605,1.14813,-0.32,-0.291,-0.261,-0.25,0.091,0.375,-1.121,2.157,0.664,-0.123,2.174,0.957316,0.610639,0.520922,1.32187,1.15613,-0.730225,-0.730225,-0.318,-0.04,0.002,0.156],
[-0.536729,0.31,-0.841,1.36339,1.19817,-0.619334,1.432,-0.276,-0.196,-0.258,-0.581,-0.831,-0.677,0.869,-0.83,-0.420482,-0.483,-0.967,0.495,-0.189301,1.65095,0.246694,-0.303,-0.298,-0.205,0.41,-1.281,-1.276,1.015,-1.212,-0.518,-0.139,-1.214,0.942297,0.109461,0.451311,0.958245,0.271001,2.43995,2.43995,-0.321,-0.308,-0.316,-0.004],
[0.460732,-0.063,-0.396,0.992239,0.676232,1.28903,-0.454,-0.268,-0.104,-0.115,0.123,-0.687,-0.479,0.92,-1.135,0.477877,-0.801,0.492,0.157,-0.709293,2.11824,0.456616,-0.281,-0.312,-0.204,0.656,0.272,1.775,-0.094,0.854,0.507,-0.519,-0.044,0.572005,-0.533806,-0.459432,2.06123,1.32209,2.15545,2.15545,-0.309,-0.32,-0.299,0.421],
[0.710098,0.755,0.781,1.80665,2.11157,-0.0295973,-0.543,-0.275,0.03,-0.183,-0.4,0.548,1.022,-0.309,1.456,1.15165,0.912,0.775,1.977,1.89651,-0.322056,-0.407772,-0.321,-0.174,-0.236,-0.352,0.942,1.527,1.015,-1.212,0.999,0.933,1.407,1.98585,1.86479,1.59989,0.120399,-0.128534,-0.262827,-0.262827,-0.324,-0.147,-0.275,-0.289],
[0.211367,0.648,1.616,1.70061,1.86433,0.334847,-0.712,-0.274,0.157,-0.091,-0.258,1.2,1.197,-1.129,1.246,-0.0461656,0.338,-0.454,1.366,1.01428,-0.847758,2.45706,-0.322,-0.304,-0.27,-0.661,-0.161,0.275,1.015,-1.212,0.093,0.53,0.971,1.59329,1.39261,1.35625,0.488567,0.148067,-0.323792,-0.323792,-0.323,-0.148,-0.252,-0.173],
[1.52944,0.215,0.479,1.25947,1.46601,0.719169,-0.73,-0.272,0.131,-0.031,-0.114,-0.093,0.326,0.185,1.081,1.11421,-0.143,0.073,0.848,0.476757,0.35292,1.28396,-0.316,-0.301,-0.253,0.049,-0.525,-0.084,0.184,0.511,1.433,0.021,0.001,1.08834,0.546361,0.770361,0.791585,0.314028,1.13937,1.13937,-0.322,-0.285,-0.304,-0.048],
[0.852592,-0.171,0.663,0.885135,0.662497,0.666159,0.4,-0.271,-0.183,-0.222,0.008,-0.111,0.55,-0.654,1.354,1.00192,-0.346,-0.65,0.64,-0.317838,-0.00403831,3.6672,-0.314,-0.318,-0.265,0.14,-1.095,-0.576,-0.015,0.749,0.999,-0.296,0.03,0.789519,0.0215013,0.207673,1.24763,0.762738,1.66773,1.66773,-0.319,-0.305,-0.302,0.15],
[0.353861,-0.674,0.342,0.384609,0.00320489,1.90527,-0.41,-0.259,-0.153,-0.09,0.372,-0.398,-0.032,-0.242,0.917,-0.869661,-0.729,1.067,0.234,-0.0549205,1.69638,-0.123758,-0.305,-0.283,-0.177,0.38,-0.236,-0.037,-0.839,1.111,-0.262,-0.78,0.365,0.318238,-0.731474,-0.732075,2.72636,2.01052,0.834543,0.834543,-0.296,-0.316,-0.255,0.621],
[1.45819,-0.11,-0.964,0.937096,-0.635484,-0.400667,3.497,-0.269,-0.227,-0.26,0.06,-0.926,-0.779,0.485,0.608,0.814761,-0.729,-0.424,0.227,-1.01311,0.975974,3.24735,-0.273,-0.323,-0.257,0.72,-1.083,-0.828,-0.286,0.955,1.236,-0.499,-0.861,0.584434,-0.821367,-0.737876,1.81124,1.00246,4.14697,4.14697,-0.306,-0.328,-0.316,0.487],
[1.20883,-1.55,0.901,-0.492372,-0.587411,2.35585,-0.267,-0.24,-0.196,-0.104,0.652,0.064,1.177,-1.791,1.195,1.30137,-0.859,1.702,0.09,-0.0432353,1.89758,-0.555952,-0.304,-0.247,-0.084,0.4,0.194,0.257,-1.414,1.232,1.354,-1.297,1.722,-0.190331,-0.705376,-0.755279,1.59307,2.24409,0.102963,0.102963,-0.293,-0.306,-0.195,0.657],
[0.318238,-1.794,-0.199,-0.730971,-1.2261,2.89921,-0.152,-0.019,-0.227,-0.104,1.413,-0.818,-0.657,-1.352,0.835,1.03935,-2.09,-0.271,-1.212,-1.34614,3.27998,-0.407772,5.869,-0.325,-0.058,2.294,-1.362,-1.915,-2.431,1.199,0.723,-2.168,-0.672,-1.03346,-1.36459,-1.43979,1.82336,3.30133,-0.953763,-0.953763,4.949,-0.332,1.766,2.417],
[-0.0736222,0.205,-0.944,1.24992,0.415262,-0.778364,2.642,-0.276,-0.216,-0.262,-0.582,-0.76,-0.578,0.863,-0.806,0.32815,0.212,-0.732,1.224,1.03765,-0.698484,2.09895,-0.321,-0.302,-0.269,-0.487,-0.586,-0.165,0.985,-0.9,0.132,0.232,-0.771,1.29395,0.758045,1.0488,-0.506849,-0.607977,3.21218,3.21218,-0.325,-0.306,-0.33,-0.454],
[-1.17795,0.5,-0.355,1.54261,1.54842,-0.36091,0.614,-0.276,-0.17,-0.249,-0.471,-0.317,0.354,0.892,-0.944,1.11421,0.212,-0.32,1.223,0.727989,-0.458349,2.29653,-0.32,-0.306,-0.266,-0.273,-0.136,0.165,1.015,-1.212,-0.065,0.379,-0.447,1.43637,1.23844,1.55348,-0.432609,-0.552657,1.2613,1.2613,-0.325,-0.272,-0.324,-0.481],
[1.1732,0.114,1.78,1.16827,0.916599,1.10349,-0.516,-0.27,-0.07,-0.108,0.047,1.545,1.571,-0.83,0.882,1.41367,-0.886,0.882,0.067,-0.487274,1.46923,1.22222,-0.298,-0.314,-0.236,0.485,-0.44,0.676,-1.251,1.33,1.394,-0.482,1.093,0.607739,0.073214,0.178668,1.43853,1.24218,0.18425,0.18425,-0.316,-0.271,-0.243,0.236],
[0.425109,1.236,-0.197,2.26158,2.32446,-1.03016,0.489,-0.278,-0.147,-0.263,-1.354,0.74,0.674,0.939,-1.473,-0.38305,0.534,2.315,1.555,1.14281,0.78776,-0.975798,-0.317,0.457,0.343,0.035,1.929,0.067,-0.343,-0.094,0.034,0.943,1.818,1.97238,1.5289,1.0314,1.2552,0.510723,-0.466043,-0.466043,-0.322,-0.13,-0.202,-0.041],
[-0.287364,-0.334,-0.266,0.722888,-0.326441,0.778806,1.53,-0.262,-0.218,-0.24,0.298,-0.524,-0.212,0.17,0.474,-0.15846,0.223,0.743,1.245,1.18956,0.203646,0.0861644,-0.319,-0.257,-0.237,-0.095,0.095,0.51,0.086,0.934,-0.242,-0.03,0.459,1.04795,0.523646,0.718153,0.441599,0.227974,1.64741,1.64741,-0.322,-0.296,-0.311,-0.061],
[0.0332487,1.2,1.682,1.90103,1.54155,0.0167865,-0.383,-0.275,-0.07,-0.202,-0.313,2.379,0.762,-0.58,0.902,-0.345618,0.855,-0.215,1.602,2.03673,-0.763386,-0.580649,-0.322,-0.113,-0.253,-0.667,0.231,0.752,1.015,-1.212,-0.164,1.121,0.895,1.82271,1.91167,1.69271,-0.746233,-0.706325,0.143606,0.143606,-0.326,-0.207,-0.319,-0.611],
[-1.35607,0.148,-0.972,1.19372,-0.0654714,-0.818121,3.319,-0.276,-0.222,-0.263,-0.503,-0.569,-0.356,0.939,-1.473,-1.13168,1.553,-0.687,2.644,2.74369,-0.984051,-0.963449,-0.323,0.816,-0.251,-1.189,1.4,0.404,0.256,0.318,-1.346,1.02,-1.052,2.05887,2.48245,2.63245,-1.23864,-1.05054,-0.872477,-0.872477,-0.326,0.752,-0.321,-1.39],
[-0.786095,1.4,1.007,2.44928,2.16651,-0.208506,-0.374,-0.276,-0.038,-0.215,-0.476,1.86,1.347,0.248,0.418,-0.15846,1.295,-0.231,2.383,2.38145,-0.97107,-0.197849,-0.322,-0.193,-0.269,-1.104,1.182,0.538,1.015,-1.212,-0.518,1.489,0.8,2.52653,2.6951,2.267,-0.779565,-0.773939,-0.466043,-0.466043,-0.326,-0.028,-0.308,-0.734],
[-0.928589,0.25,-0.842,1.30401,0.42213,-0.818121,2.705,-0.276,-0.216,-0.263,-0.63,-0.572,-0.201,0.882,-0.891,-1.80545,1.095,-0.438,2.171,2.30549,-0.977561,-0.0249712,-0.322,-0.217,-0.27,-1.123,0.766,0.248,1.015,-1.212,-1.465,0.792,-0.631,1.84861,1.97933,2.15678,-0.821988,-0.810819,0.0419981,0.0419981,-0.326,-0.175,-0.322,-0.764],
[-0.144869,1.038,-0.001,2.07707,2.25579,-0.824748,0.32,-0.277,-0.138,-0.258,-0.919,0.445,1.12,0.446,0.645,-1.28141,-0.071,0.172,0.925,0.581924,0.300999,1.16048,-0.317,-0.298,-0.252,0.019,-0.395,0.347,0.264,0.283,-0.755,0.476,-0.096,1.53062,1.30127,1.50128,-0.596239,-0.620271,1.66773,1.66773,-0.325,-0.282,-0.327,-0.521],
[1.1732,0.314,1.073,1.35809,1.12263,0.546887,-0.054,-0.272,-0.142,-0.205,-0.101,0.869,1.02,-0.397,0.93,0.215856,-0.397,1.444,0.579,0.313164,1.16419,0.0861644,-0.312,-0.281,-0.209,0.228,0.155,0.738,-0.796,1.276,0.763,-0.085,1.458,0.986318,0.540561,0.57313,1.04461,0.793471,0.245215,0.245215,-0.32,-0.259,-0.264,0.086],
[-1.17795,1.526,-0.753,2.56168,3.32714,-1.12956,-0.659,-0.278,0.244,-0.265,-2.548,0.578,0.426,0.939,-1.473,-0.60764,0.813,0.73,1.861,1.72123,-0.432388,0.147906,-0.321,-0.247,-0.256,-0.387,0.886,1.111,0.476,0.114,-0.972,1.258,-0.053,2.28985,2.55639,2.41782,-1.15834,-1.00751,-0.262827,-0.262827,-0.326,-0.097,-0.329,-1.171],
[-0.394235,-0.548,-0.958,0.502317,-1.23297,1.2824,2.011,-0.114,-0.231,-0.238,1.231,-1.036,-1.19,0.592,-0.115,-1.54343,1.776,-0.476,2.88,2.68526,-1.04895,-0.716482,-0.323,0.024,-0.272,-1.968,1.929,0.714,1.015,-1.212,-1.031,0.809,-0.913,1.85327,2.02573,2.22059,-0.965921,-0.896873,0.0623198,0.0623198,-0.326,-0.175,-0.326,-0.89],
[0.63885,0.382,0.056,1.42596,1.23251,0.0499178,0.471,-0.274,-0.172,-0.238,-0.257,-0.045,0.772,0.535,-0.143,0.515308,0.551,2.227,1.583,1.38236,0.521664,-0.951101,-0.318,0.361,0.149,-0.045,1.505,1.148,-0.654,-0.448,0.625,0.527,1.092,1.58034,1.44867,1.40846,0.614319,0.252561,-0.892798,-0.892798,-0.323,0.521,0.07,-0.151],
[-1.9973,-0.168,-0.885,0.888316,-0.360779,0.195695,2.358,-0.267,-0.222,-0.252,0.168,-1.036,-1.192,0.701,-0.368,-2.2172,0.055,0.571,1.066,0.827314,0.482723,0.320784,-0.317,-0.277,-0.236,0.021,0.007,0.396,0.276,0.165,-2.273,-0.05,-0.358,1.02878,0.412971,0.671745,1.05976,0.443109,1.09872,1.09872,-0.321,-0.287,-0.3,-0.002],
[-1.21358,0.143,0.156,1.18842,0.834187,0.507129,0.373,-0.272,-0.177,-0.225,-0.061,-0.097,0.787,0.422,0.206,-0.719935,0.393,1.835,1.415,1.02012,0.177686,0.468965,-0.318,-0.278,-0.246,-0.079,1.261,1.064,-0.83,0.688,-1.051,0.311,0.832,1.37059,0.995343,1.0314,0.670378,0.271001,0.367145,0.367145,-0.323,-0.25,-0.288,-0.097]])



X = coupleValuesEntire

matLevel = np.array([0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2])
matReal = np.array([0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2])
matCat3Short = np.array([0,0,1,1,1,1,0,0,1,1,0,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])
matCat3Long = np.array([0,0,2,1,0,0,0,0,0,0,0,0,2,0,2,2,2,1,2,2,0,1,2,2,2,2,2,2,1,1])
firstHalfShort = np.array([0,0,2,0,0,0,1,0,0,2,1,0,2,1,2,2,0,2,2,0,0,0,2,2,2,2,2,1,0,2])
firstHalfLong = np.array([0,1,2,1,0,1,0,0,0,2,0,1,2,0,2,2,0,1,2,1,0,1,2,2,2,2,2,2,0,1])
secondHalfShort = np.array([0,1,2,0,1,0,0,0,0,0,1,0,1,0,1,0,2,1,2,2,1,1,2,2,2,2,2,2,2,1])

y = matLevel
#y = secondHalfShort

#X = coupleValuesEntire[:,[0,2,13,15,17,19]]
X = coupleValuesEntire[:,[10,25,43]]


# Some noisy data not correlated
E = np.random.uniform(0, 0.1, size=(len(coupleValuesEntire), 3))

# Add the noisy data to the informative features
X = np.hstack((X, E))

###############################################################################
pl.figure(1)
pl.clf()

X_indices = np.arange(X.shape[-1])

###############################################################################
# Univariate feature selection with F-test for feature scoring
# We use the default selection function: the 10% most significant features
selector = SelectPercentile(f_classif, percentile=10)
selector.fit(X, y)
scores = -np.log10(selector.pvalues_)
scores /= scores.max()
pl.bar(X_indices - .45, scores, width=.2,
       label=r'Univariate score ($-Log(p_{value})$)', color='g')

###############################################################################
# Compare to the weights of an SVM
clf = svm.SVC(kernel='linear')
clf.fit(X, y)

svm_weights = (clf.coef_ ** 2).sum(axis=0)
svm_weights /= svm_weights.max()

pl.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight', color='r')

clf_selected = svm.SVC(kernel='linear')
clf_selected.fit(selector.transform(X), y)

svm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)
svm_weights_selected /= svm_weights_selected.max()

pl.bar(X_indices[selector.get_support()] - .05, svm_weights_selected, width=.2,
       label='SVM weights after selection', color='b')


pl.title("Comparing feature selection")
pl.xlabel('Feature number')
pl.yticks(())
pl.axis('tight')
pl.legend(loc='upper right')
pl.show()
